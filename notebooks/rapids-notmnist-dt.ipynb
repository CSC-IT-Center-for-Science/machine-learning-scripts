{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# notMNIST letters classification with ensembles of decision trees \n",
    "\n",
    "In this notebook, we'll use two different ensembles of decision trees: [random forest](https://docs.rapids.ai/api/cuml/stable/api.html#random-forest) and [gradient boosted trees](https://xgboost.readthedocs.io/en/latest/) to classify notMNIST letters using a GPU, the [RAPIDS](https://rapids.ai/) libraries (cudf, cuml) and [XGBoost](https://xgboost.readthedocs.io/en/latest/).\n",
    "\n",
    "**Note that a GPU is required with this notebook.**\n",
    "\n",
    "This version of the notebook has been tested with RAPIDS version 0.15.\n",
    "\n",
    "First, the needed imports. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from pml_utils import show_failures\n",
    "\n",
    "import cudf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "from cuml.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from cuml import __version__ as cuml_version\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn import __version__ as sklearn_version\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "print('Using cudf version:', cudf.__version__)\n",
    "print('Using cuml version:', cuml_version)\n",
    "print('Using xgboost version:', xgb.__version__)\n",
    "print('Using sklearn version:', sklearn_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we load the notMNIST data. First time we need to download the data, which can take a while. The data is stored as Numpy arrays in host (CPU) memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_not_mnist(directory, filename):\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    if os.path.isfile(filepath):\n",
    "        print('Not downloading, file already exists:', filepath)\n",
    "    else:\n",
    "        if not os.path.isdir(directory):\n",
    "            os.mkdir(directory)\n",
    "        url_base = 'https://a3s.fi/mldata/'\n",
    "        url = url_base + filename\n",
    "        print('Downloading {} to {}'.format(url, filepath))\n",
    "        urllib.request.urlretrieve(url, filepath)\n",
    "    return np.load(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load notMNIST\n",
    "DATA_DIR = os.path.expanduser('~/data/notMNIST/')\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "    \n",
    "X_train = load_not_mnist(DATA_DIR, 'notMNIST_large_images.npy').reshape(-1, 28*28)\n",
    "y_train = load_not_mnist(DATA_DIR, 'notMNIST_large_labels.npy')\n",
    "X_test = load_not_mnist(DATA_DIR, 'notMNIST_small_images.npy').reshape(-1, 28*28)\n",
    "y_test = load_not_mnist(DATA_DIR, 'notMNIST_small_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n",
    "\n",
    "print()\n",
    "print('notMNIST data loaded: train:',len(X_train),'test:',len(X_test))\n",
    "print('X_train:', type(X_train), 'shape:', X_train.shape, X_train.dtype)\n",
    "print('y_train:', type(y_train), 'shape:', y_train.shape, y_train.dtype)\n",
    "print('X_test:', type(X_test), 'shape:', X_test.shape)\n",
    "print('y_test:', type(y_test), 'shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest\n",
    "\n",
    "Random forest is an ensemble (or a group; hence the name *forest*) of decision trees, obtained by introducing randomness into the tree generation. The prediction of the random forest is obtained by *averaging* the predictions of the individual trees.\n",
    "\n",
    "### Data\n",
    "\n",
    "Let's convert our training data to cuDF DataFrames in device (GPU) memory. We will also convert the classes in `y_train` to integers in \n",
    "$[0 \\mathrel{{.}\\,{.}} 9]$. \n",
    "\n",
    "We do not explicitly need to convert the test data as the GPU-based inference functionality will take care of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cu_X_train = cudf.DataFrame.from_pandas(pd.DataFrame(X_train))\n",
    "cu_y_train = cudf.Series(y_train.view(np.int32)-ord('A'))\n",
    "\n",
    "print('cu_X_train:', type(cu_X_train), 'shape:', cu_X_train.shape)\n",
    "print('cu_y_train:', type(cu_y_train), 'shape:', cu_y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning\n",
    "\n",
    "Random forest classifiers are quick to train, quite robust to hyperparameter values, and often work relatively well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "n_estimators = 100\n",
    "max_depth = 16\n",
    "clf_rf = RandomForestClassifier(n_estimators=n_estimators,\n",
    "                                max_depth=max_depth)\n",
    "clf_rf.fit(cu_X_train, cu_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "We will use GPU-based inference to predict the classes for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "pred_rf = clf_rf.predict(X_test, predict_model='GPU')\n",
    "pred_rf = [chr(x) for x in pred_rf+ord('A')]\n",
    "pred_rf = np.array(pred_rf)\n",
    "\n",
    "print('Predicted', len(pred_rf), 'digits with accuracy:',\n",
    "      accuracy_score(y_test, pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Failure analysis\n",
    "\n",
    "The random forest classifier worked quite well, so let's take a closer look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the first 10 test digits the random forest model classified to a wrong class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "show_failures(pred_rf, y_test, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `show_failures()` to inspect failures in more detail. For example:\n",
    "\n",
    "* show failures in which the true class was \"5\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "show_failures(pred_rf, y_test, X_test, trueclass=\"F\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* show failures in which the prediction was \"A\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "show_failures(pred_rf, y_test, X_test, predictedclass=\"A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* show failures in which the true class was \"A\" and the prediction was \"C\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "show_failures(pred_rf, y_test, X_test, trueclass=\"A\", predictedclass=\"C\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrix, accuracy, precision, and recall\n",
    "\n",
    "We can also compute the confusion matrix to see which digits get mixed the most, and look at classification accuracies separately for each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "labels=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\n",
    "print('Confusion matrix (rows: true classes; columns: predicted classes):'); print()\n",
    "cm=confusion_matrix(y_test, pred_rf, labels=labels)\n",
    "df_cm = pd.DataFrame(cm, columns=labels, index=labels)\n",
    "print(df_cm.to_string()); print()\n",
    "\n",
    "print('Classification accuracy for each class:'); print()\n",
    "for i,j in enumerate(cm.diagonal()/cm.sum(axis=1)): print(\"%d: %.4f\" % (i,j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision and recall for each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred_rf, labels=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient boosted trees (XGBoost)\n",
    "\n",
    "Gradient boosted trees (or extreme gradient boosted trees) is another way of constructing ensembles of decision trees, using the *boosting* framework.  Here we use the GPU accelerated [XGBoost](http://xgboost.readthedocs.io/en/latest/) library to train gradient boosted trees to classify MNIST digits. \n",
    "\n",
    "### Data\n",
    "\n",
    "We begin by converting our training and test data to XGBoost's internal `DMatrix` data structures. \n",
    "\n",
    "We will also convert the classes in `y_train` and `y_test` to integers in $[0 \\mathrel{{.}\\,{.}} 9]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train, label=y_train.view(np.int32)-ord('A'))\n",
    "dtest = xgb.DMatrix(X_test, label=y_test.view(np.int32)-ord('A'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning\n",
    "\n",
    "XGBoost has been used to obtain record-breaking results on many machine learning competitions, but have quite a lot of hyperparameters that need to be carefully tuned to get the best performance.\n",
    "\n",
    "For more information, see the documentation for [XGBoost Parameters](https://xgboost.readthedocs.io/en/latest/parameter.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T21:03:57.443698Z",
     "start_time": "2018-11-06T21:03:57.438288Z"
    }
   },
   "outputs": [],
   "source": [
    "# instantiate params\n",
    "params = {}\n",
    "\n",
    "# general params\n",
    "general_params = {'verbosity': 2}\n",
    "params.update(general_params)\n",
    "\n",
    "# booster params\n",
    "booster_params = {'tree_method': 'gpu_hist'}\n",
    "params.update(booster_params)\n",
    "\n",
    "# learning task params\n",
    "learning_task_params = {'objective': 'multi:softmax', 'num_class': 10}\n",
    "params.update(learning_task_params)\n",
    "\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specify the number of boosted trees and are then ready to train our gradient boosted trees model on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "num_round = 100\n",
    "clf_xgb = xgb.train(params, dtrain, num_round)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Inference is also run on the GPU. \n",
    "\n",
    "To match `y_test`, we also convert the predicted integer classes back to letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "pred_xgb = clf_xgb.predict(dtest)\n",
    "pred_xgb = [chr(x) for x in pred_xgb.astype(np.int32)+ord('A')]\n",
    "pred_xgb = np.array(pred_xgb)\n",
    "print('Predicted', len(pred_xgb), 'letters with accuracy:', accuracy_score(y_test, pred_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use `show_failures()` to inspect the failures, and calculate the confusion matrix and other metrics as was done with the random forest above.\n",
    "\n",
    "## Model tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study the documentation of the different decision tree models used in this notebook ([cuml random forest](https://docs.rapids.ai/api/cuml/stable/api.html#random-forest) and [XGBoost gradient boosted trees](https://xgboost.readthedocs.io/en/latest/)), and experiment with different hyperparameter values.  \n",
    "\n",
    "Report the highest classification accuracy you manage to obtain for each model type.  Also mark down the parameters you used, so others can try to reproduce your results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
