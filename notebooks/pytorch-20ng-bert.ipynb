{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jNKaJz5j_ylj"
   },
   "source": [
    "# 20 newsgroup text classification with BERT finetuning\n",
    "\n",
    "In this notebook, we'll use a pre-trained [BERT](https://arxiv.org/abs/1810.04805) model for text classification using PyTorch and HuggingFace's [Transformers](https://github.com/huggingface/transformers). This notebook is based on [\"Predicting Movie Review Sentiment with BERT on TF Hub\"](https://github.com/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb) by Google and [\"BERT Fine-Tuning Tutorial with PyTorch\"](https://mccormickml.com/2019/07/22/BERT-fine-tuning/) by Chris McCormick.\n",
    "\n",
    "**Note that using a GPU with this notebook is highly recommended.**\n",
    "\n",
    "First, the needed imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Ok002ceNB8E7",
    "outputId": "06ef90d2-7518-4209-da66-1dd45c357c78"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import (TensorDataset, DataLoader,\n",
    "                              RandomSampler, SequentialSampler)\n",
    "\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import __version__ as transformers_version\n",
    "\n",
    "from distutils.version import LooseVersion as LV\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import io, sys, os, datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    devicename = '['+torch.cuda.get_device_name(0)+']'\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    devicename = \"\"\n",
    "    \n",
    "print('Using PyTorch version:', torch.__version__,\n",
    "      'Transformers version:', transformers_version,\n",
    "      'Device:', device, devicename)\n",
    "assert(LV(torch.__version__) >= LV(\"1.0.0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20 Newsgroups data set\n",
    "\n",
    "Next we'll load the [20 Newsgroups](http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.html) data set. \n",
    "\n",
    "The dataset contains 20000 messages collected from 20 different Usenet newsgroups (1000 messages from each group):\n",
    "\n",
    "|[]()|[]()|[]()|[]()|\n",
    "| --- | --- |--- | --- |\n",
    "| alt.atheism           | soc.religion.christian   | comp.windows.x     | sci.crypt |               \n",
    "| talk.politics.guns    | comp.sys.ibm.pc.hardware | rec.autos          | sci.electronics |              \n",
    "| talk.politics.mideast | comp.graphics            | rec.motorcycles    | sci.space |                   \n",
    "| talk.politics.misc    | comp.os.ms-windows.misc  | rec.sport.baseball | sci.med |                     \n",
    "| talk.religion.misc    | comp.sys.mac.hardware    | rec.sport.hockey   | misc.forsale |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_DATA_DIR = \"/media/data/20_newsgroup\"\n",
    "\n",
    "print('Processing text dataset')\n",
    "\n",
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "    path = os.path.join(TEXT_DATA_DIR, name)\n",
    "    if os.path.isdir(path):\n",
    "        label_id = len(labels_index)\n",
    "        labels_index[name] = label_id\n",
    "        for fname in sorted(os.listdir(path)):\n",
    "            if fname.isdigit():\n",
    "                fpath = os.path.join(path, fname)\n",
    "                args = {} if sys.version_info < (3,) else {'encoding': 'latin-1'}\n",
    "                with open(fpath, **args) as f:\n",
    "                    t = f.read()\n",
    "                    i = t.find('\\n\\n')  # skip header\n",
    "                    if 0 < i:\n",
    "                        t = t[i:]\n",
    "                    texts.append(t)\n",
    "                labels.append(label_id)\n",
    "\n",
    "print('Found %s texts.' % len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into a training set and a test set using scikit-learn's `train_test_split()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SET = 4000\n",
    "\n",
    "(sentences_train, sentences_test,\n",
    " labels_train, labels_test) = train_test_split(texts, labels, \n",
    "                                               test_size=TEST_SET,\n",
    "                                               shuffle=True, random_state=42)\n",
    "\n",
    "print('Length of training texts:', len(sentences_train))\n",
    "print('Length of training labels:', len(labels_train))\n",
    "print('Length of test texts:', len(sentences_test))\n",
    "print('Length of test labels:', len(labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The token `[CLS]` is a special token required by BERT at the beginning of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GuE5BqICAne2"
   },
   "outputs": [],
   "source": [
    "sentences_train = [\"[CLS] \" + s for s in sentences_train]\n",
    "sentences_test = [\"[CLS] \" + s for s in sentences_test]\n",
    "\n",
    "print (\"The first training sentence:\")\n",
    "print(sentences_train[0], 'LABEL:', labels_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BTREubVNFiz4"
   },
   "source": [
    "Next we specify the pre-trained BERT model we are going to use. The model `\"bert-base-uncased\"` is the lowercased \"base\" model (12-layer, 768-hidden, 12-heads, 110M parameters).\n",
    "\n",
    "We load the used vocabulary from the BERT model, and use the BERT tokenizer to convert the sentences into tokens that match the data the BERT model was trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "Z474sSC6oe7A",
    "outputId": "fbaa8fd8-bccd-4feb-ce52-beba5d293cfa"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "BERTMODEL='bert-base-uncased'\n",
    "CACHE_DIR='/media/data/transformers-cache/'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(BERTMODEL,\n",
    "                                          do_lower_case=True,\n",
    "                                          cache_dir=CACHE_DIR)\n",
    "\n",
    "tokenized_train = [tokenizer.tokenize(s) for s in sentences_train]\n",
    "tokenized_test  = [tokenizer.tokenize(s) for s in sentences_test]\n",
    "\n",
    "print (\"The full tokenized first training sentence:\")\n",
    "print (tokenized_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cp9BPRd1tMIo"
   },
   "source": [
    "Now we set the maximum sequence lengths for our training and test sentences as `MAX_LEN_TRAIN` and `MAX_LEN_TEST`. The maximum length supported by the used BERT model is 512.\n",
    "\n",
    "The token `[SEP]` is another special token required by BERT at the end of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cp9BPRd1tMIo"
   },
   "outputs": [],
   "source": [
    "MAX_LEN_TRAIN, MAX_LEN_TEST = 128, 512\n",
    "\n",
    "tokenized_train = [t[:(MAX_LEN_TRAIN-1)]+['SEP'] for t in tokenized_train]\n",
    "tokenized_test  = [t[:(MAX_LEN_TEST-1)]+['SEP'] for t in tokenized_test]\n",
    "\n",
    "print (\"The truncated tokenized first training sentence:\")\n",
    "print (tokenized_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use the BERT tokenizer to convert each token into an integer index in the BERT vocabulary. We also pad any shorter sequences to `MAX_LEN_TRAIN` or `MAX_LEN_TEST` indices with trailing zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_train = [tokenizer.convert_tokens_to_ids(t) for t in tokenized_train]\n",
    "ids_train = np.array([np.pad(i, (0, MAX_LEN_TRAIN-len(i)), \n",
    "                             mode='constant') for i in ids_train])\n",
    "\n",
    "ids_test = [tokenizer.convert_tokens_to_ids(t) for t in tokenized_test]\n",
    "ids_test = np.array([np.pad(i, (0, MAX_LEN_TEST-len(i)), \n",
    "                            mode='constant') for i in ids_test])\n",
    "\n",
    "print (\"The indices of the first training sentence:\")\n",
    "print (ids_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KhGulL1pExCT"
   },
   "source": [
    "BERT also requires *attention masks*, with 1 for each real token in the sequences and 0 for the padding:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cDoC24LeEv3N"
   },
   "outputs": [],
   "source": [
    "amasks_train, amasks_test = [], []\n",
    "\n",
    "for seq in ids_train:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  amasks_train.append(seq_mask)\n",
    "    \n",
    "for seq in ids_test:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  amasks_test.append(seq_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aFbE-UHvsb7-"
   },
   "source": [
    "We use again scikit-learn's `train_test_split` to use 10% of our training data as a validation set, and then convert all data into `torch.tensor`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aFbE-UHvsb7-"
   },
   "outputs": [],
   "source": [
    "(train_inputs, validation_inputs, \n",
    " train_labels, validation_labels) = train_test_split(ids_train, labels_train, \n",
    "                                                     random_state=42,\n",
    "                                                     test_size=0.1)\n",
    "(train_masks, validation_masks, \n",
    " _, _) = train_test_split(amasks_train, ids_train,\n",
    "                          random_state=42, test_size=0.1)\n",
    "\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "train_masks  = torch.tensor(train_masks)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "validation_masks  = torch.tensor(validation_masks)\n",
    "test_inputs = torch.tensor(ids_test)\n",
    "test_labels = torch.tensor(labels_test)\n",
    "test_masks  = torch.tensor(amasks_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create PyTorch *DataLoader*s for all data sets. \n",
    "\n",
    "For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GEgLpFVlo1Z-"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "print('Train: ', end=\"\")\n",
    "train_data = TensorDataset(train_inputs, train_masks,\n",
    "                           train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, \n",
    "                              batch_size=BATCH_SIZE)\n",
    "print(len(train_data), 'articles')\n",
    "\n",
    "print('Validation: ', end=\"\")\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks,\n",
    "                                validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data,\n",
    "                                   sampler=validation_sampler,\n",
    "                                   batch_size=BATCH_SIZE)\n",
    "print(len(validation_data), 'articles')\n",
    "\n",
    "print('Test: ', end=\"\")\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler,\n",
    "                             batch_size=BATCH_SIZE)\n",
    "print(len(test_data), 'articles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "gFsCTp_mporB",
    "outputId": "dd067229-1925-4b37-f517-0c14e25420d1"
   },
   "source": [
    "## BERT model initialization\n",
    "\n",
    "We now load a pretrained BERT model with a single linear classification layer added on top. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "gFsCTp_mporB",
    "outputId": "dd067229-1925-4b37-f517-0c14e25420d1"
   },
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(BERTMODEL,\n",
    "                                                      cache_dir=CACHE_DIR,\n",
    "                                                      num_labels=20)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8o-VEBobKwHk"
   },
   "source": [
    "We set the remaining hyperparameters needed for fine-tuning the pretrained model:\n",
    "  * `EPOCHS`: the number of training epochs in fine-tuning (recommended values between 2 and 4)\n",
    "  * `WEIGHT_DECAY`: weight decay for the Adam optimizer\n",
    "  * `LR`: learning rate for the Adam optimizer (2e-5 to 5e-5 recommended)\n",
    "  * `WARMUP_STEPS`: number of warmup steps to (linearly) reach the set learning rate \n",
    "  \n",
    "We also need to grab the training parameters from the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QxSMw0FrptiL"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 4\n",
    "WEIGHT_DECAY = 0.01 \n",
    "LR = 2e-5\n",
    "WARMUP_STEPS =int(0.2*len(train_dataloader))\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay': WEIGHT_DECAY},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=LR, eps=1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS,\n",
    "                                            num_training_steps=len(train_dataloader)*EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning\n",
    "\n",
    "Let's now define functions to `train()` and `evaluate()` the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 172
    },
    "colab_type": "code",
    "id": "6J-FYdx6nFE_",
    "outputId": "8e388ad1-f9db-4c7b-d080-6c0a0e964610"
   },
   "outputs": [],
   "source": [
    "def train(epoch, loss_vector=None, log_interval=200):\n",
    "  # Set model to training mode\n",
    "  model.train()\n",
    "  \n",
    "  # Loop over each batch from the training set\n",
    "  for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "    # Copy data to GPU if needed\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    \n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    \n",
    "    # Zero gradient buffers\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(b_input_ids, token_type_ids=None, \n",
    "                    attention_mask=b_input_mask, labels=b_labels)\n",
    "\n",
    "    loss = outputs[0]\n",
    "    if loss_vector is not None:\n",
    "        loss_vector.append(loss.item())\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update weights\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    if step % log_interval == 0:\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, step * len(b_input_ids),\n",
    "                len(train_dataloader.dataset),\n",
    "                100. * step / len(train_dataloader), loss))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 172
    },
    "colab_type": "code",
    "id": "6J-FYdx6nFE_",
    "outputId": "8e388ad1-f9db-4c7b-d080-6c0a0e964610"
   },
   "outputs": [],
   "source": [
    "def evaluate(loader):\n",
    "  model.eval()\n",
    "\n",
    "  n_correct, n_all = 0, 0\n",
    "    \n",
    "  for batch in loader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "      outputs = model(b_input_ids, token_type_ids=None,\n",
    "                      attention_mask=b_input_mask)\n",
    "      logits = outputs[0]\n",
    "    \n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "\n",
    "    labels = b_labels.to('cpu').numpy()\n",
    "    n_correct += np.sum(predictions == labels)\n",
    "    n_all += len(labels)\n",
    "    \n",
    "  print('Accuracy: [{}/{}] {:.4f}\\n'.format(n_correct, n_all, n_correct/n_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train our model using the `train()` function. After each epoch, we evaluate the model using the validation set and `evaluate()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_lossv = []\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(epoch, train_lossv)\n",
    "    print('\\nValidation set:')\n",
    "    evaluate(validation_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1-G03mmwH3aI"
   },
   "source": [
    "Let's take a look at our training loss over all batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 512
    },
    "colab_type": "code",
    "id": "68xreA9JAmG5",
    "outputId": "6861a923-7a4e-4d03-b6fa-3038b6e2f89d"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(train_lossv, label='original')\n",
    "plt.plot(np.convolve(train_lossv, np.ones(101), 'same') / 101, label='averaged')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mkyubuJSOzg3"
   },
   "source": [
    "## Inference\n",
    "\n",
    "For a better measure of the quality of the model, let's see the model accuracy for the test messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print('Test set:')\n",
    "evaluate(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT Fine-Tuning Sentence Classification.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
